{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squeze/my_udacity_deep_learning_solutions/blob/master/intro-neural-networks/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z0eKFKr8neG",
        "colab_type": "text"
      },
      "source": [
        "# Implementing the gradient descent algorithm\n",
        "\n",
        "\n",
        "This notebook is based on the udacity deep learning nanodegree exercise for gradient descent, which can be found here:\n",
        "\n",
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-neural-networks/gradient-descent/GradientDescent.ipynb\n",
        "\n",
        "The original version is implemented with python and numpy, I try to implement it with swift-only as an exercise to learn swift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb5V5Sb09V4W",
        "colab_type": "text"
      },
      "source": [
        "## Loading dataset from github\n",
        "The original dataset is located here:\n",
        "\n",
        "https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-neural-networks/gradient-descent/data.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Foundation\n",
        "\n",
        "let url = \"https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-neural-networks/gradient-descent/data.csv\"\n",
        "\n",
        "// author of this query function: https://gist.github.com/groz/85b95f663f79ba17946269ea65c2c0f4\n",
        "func query(address: String) -> String {\n",
        "    let url = URL(string: address)\n",
        "    let semaphore = DispatchSemaphore(value: 0)\n",
        "    \n",
        "    var result: String = \"\"\n",
        "    \n",
        "    let task = URLSession.shared.dataTask(with: url!) {(data, response, error) in\n",
        "        result = String(data: data!, encoding: String.Encoding.utf8)!\n",
        "        semaphore.signal()\n",
        "    }\n",
        "    \n",
        "    task.resume()\n",
        "    semaphore.wait()\n",
        "    return result\n",
        "}\n",
        "\n",
        "let rawData = query(address: url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBCYDv0gPiHI",
        "colab_type": "text"
      },
      "source": [
        "### Convert data\n",
        "Make features a Tensor (not only an array)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QThtcqTjMRyi",
        "colab_type": "code",
        "outputId": "88e79184-c924-4647-9408-72ac35bb10ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import TensorFlow\n",
        "\n",
        "let rows = rawData.components(separatedBy: \"\\n\")\n",
        "let featuresAndTargetsAsString = rows.map({ $0.components(separatedBy: \",\") }).filter {$0[0] != \"\"}\n",
        "var targets = [Double]()               \n",
        "var features = [Tensor<Double>]()\n",
        "for featureWithTarget in featuresAndTargetsAsString {\n",
        "  targets.append(Double(featureWithTarget[2])!)\n",
        "  features.append(Tensor<Double>([Double(featureWithTarget[0])!, Double(featureWithTarget[1])!]))\n",
        "  \n",
        "}\n",
        "print(features)\n",
        "print(targets)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0.78051, -0.063669], [0.28774, 0.29139], [0.40714, 0.17878], [0.2923, 0.4217], [0.50922, 0.35256], [0.27785, 0.10802], [0.27527, 0.33223], [0.43999, 0.31245], [0.33557, 0.42984], [0.23448, 0.24986], [0.0084492,   0.13658], [0.12419, 0.33595], [0.25644, 0.42624], [ 0.4591, 0.40426], [0.44547, 0.45117], [0.42218, 0.20118], [0.49563, 0.21445], [0.30848, 0.24306], [0.39707, 0.44438], [0.32945, 0.39217], [0.40739, 0.40271], [ 0.3106, 0.50702], [0.49638, 0.45384], [0.10073, 0.32053], [0.69907, 0.37307], [0.29767, 0.69648], [0.15099, 0.57341], [0.16427, 0.27759], [ 0.33259, 0.055964], [0.53741, 0.28637], [0.19503, 0.36879], [ 0.40278, 0.035148], [0.21296, 0.55169], [0.48447, 0.56991], [0.25476, 0.34596], [0.21726, 0.28641], [0.67078, 0.46538], [0.3815, 0.4622], [0.53838, 0.32774], [ 0.4849, 0.26071], [0.37095, 0.38809], [0.54527, 0.63911], [0.32149, 0.12007], [0.42216, 0.61666], [ 0.10194, 0.060408], [0.15254,  0.2168], [0.45558, 0.43769], [0.28488, 0.52142], [0.27633, 0.21264], [0.39748, 0.31902], [0.5533,    1.0], [0.44274, 0.59205], [0.85176,  0.6612], [0.60436, 0.86605], [0.68243, 0.48301], [    1.0, 0.76815], [0.72989,  0.8107], [0.67377, 0.77975], [0.78761, 0.58177], [0.71442,  0.7668], [0.49379, 0.54226], [0.78974, 0.74233], [0.67905, 0.60921], [ 0.6642, 0.72519], [0.79396, 0.56789], [0.70758, 0.76022], [0.59421, 0.61857], [0.49364, 0.56224], [0.77707, 0.35025], [0.79785, 0.76921], [0.70876, 0.96764], [0.69176, 0.60865], [0.66408, 0.92075], [0.65973, 0.66666], [0.64574, 0.56845], [0.89639,  0.7085], [0.85476, 0.63167], [0.62091, 0.80424], [0.79057, 0.56108], [0.58935, 0.71582], [0.56846,  0.7406], [0.65912, 0.71548], [0.70938, 0.74041], [0.59154, 0.62927], [0.45829,  0.4641], [0.79982, 0.74847], [0.60974, 0.54757], [0.68127, 0.86985], [0.76694, 0.64736], [0.69048, 0.83058], [0.68122, 0.96541], [0.73229, 0.64245], [0.76145, 0.60138], [0.58985, 0.86955], [0.73145, 0.74516], [0.77029,  0.7014], [0.73156, 0.71782], [0.44556, 0.57991], [0.85275, 0.85987], [0.51912, 0.62359]]\r\n",
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1cKth9KpfSW",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid activation function\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdTW4VQ29WZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func mySigmoid(_ x: Tensor<Double>) -> Tensor<Double> {\n",
        "  return 1 / (1 + exp(-x))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qcupn4EUj6A",
        "colab_type": "text"
      },
      "source": [
        "## Output (prediction) formula\n",
        "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWP3Fw8JJReL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func myOutputFormula(_ features: Tensor<Double>, _ weights: Tensor<Double>, _ bias: Tensor<Double>) -> Double {\n",
        "  let res = mySigmoid((features * weights).sum() + bias)\n",
        "  return res.scalar!\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD_6SeB-Ar3Z",
        "colab_type": "text"
      },
      "source": [
        "## Error function\n",
        "\n",
        "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCsbyp8JVyX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func myErrorFormula(_ y: Double, _ ŷ: Double) -> Double {\n",
        "  //y * log(ŷ) - ((1-y) * log(1-ŷ))\n",
        "  //results in:\n",
        "  //error: the compiler is unable to type-check this expression in reasonable time; try breaking up the expression into distinct sub-expressions\n",
        "  let expression1 = y * log(ŷ)\n",
        "  let expression2 = ((1-y) * log(1-ŷ))\n",
        "  return -expression1 - expression2\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLjzHBB99Jtz",
        "colab_type": "text"
      },
      "source": [
        "##Gradient descent step\n",
        "\n",
        "$$w_i \\longrightarrow w_i + \\alpha (y - \\hat{y}) x_i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5YVph7cvzNi",
        "colab_type": "text"
      },
      "source": [
        "$$b \\longrightarrow b + \\alpha (y - \\hat{y})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aiSHPAxjZ39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func myUpdateWeights(_ features: Tensor<Double>, _ targets: Double, _ weights: Tensor<Double>, _ bias: Tensor<Double>, _ learningRate: Double) -> (Tensor<Double>, Tensor<Double>) {\n",
        "  let learningRateAndYAndYHat = learningRate * (targets - myOutputFormula(features, weights, bias))\n",
        "  let updatedWeights = weights + (learningRateAndYAndYHat * features)\n",
        "  let updatedBias = bias + learningRateAndYAndYHat\n",
        "  return (updatedWeights, updatedBias)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqgcSRLE_8W_",
        "colab_type": "text"
      },
      "source": [
        "## Training function\n",
        "Initialization of weights in the course\n",
        "\n",
        "`weights = np.random.normal(scale=1 / n_features**.5, size=n_features)`\n",
        "\n",
        "is different than in my version\n",
        "\n",
        "`var weights = Tensor<Double>(randomNormal: [2])`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWYCXa5S_-34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func train(_ features: [Tensor<Double>], _ targets: [Double], epochs: Int, learningRate: Double) { //make learningRate Double because swift's won't multiply Float with Double\n",
        "  let numberRecords = Double(features.count)\n",
        "  var weights = Tensor<Double>(randomNormal: features[0].shape)\n",
        "  var bias = Tensor<Double>.zero\n",
        "  var lastLoss = Double.infinity\n",
        "  \n",
        "  for epoch in 0...epochs {\n",
        "    var errors = 0.0\n",
        "    var correctPredictions = 0.0\n",
        "    var prediction = 0.0\n",
        "\n",
        "    for (x, y) in zip(features, targets) {\n",
        "      let output = myOutputFormula(x, weights, bias)\n",
        "      errors += myErrorFormula(y, output)\n",
        "      (weights, bias) = myUpdateWeights(x, y, weights, bias, learningRate)\n",
        "\n",
        "      if (output > 0.5) {\n",
        "        prediction = 1.0\n",
        "      } else {\n",
        "        prediction = 0.0\n",
        "      }\n",
        "      \n",
        "      if (prediction == y) {\n",
        "        correctPredictions+=1\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    let loss = errors / numberRecords\n",
        "\n",
        "    if epoch % (epochs / 10) == 0 {\n",
        "      print(\"Epoch: \\(epoch)\")\n",
        "      \n",
        "      let warning = lastLoss < loss ? \"WARNING - Loss increasing\" : \"\"\n",
        "      print(\"Train loss: \\(loss) \\(warning)\")\n",
        "      lastLoss = loss\n",
        "      \n",
        "      let accuracy = correctPredictions / numberRecords\n",
        "      print(\"Accuracy: \\(accuracy)\")\n",
        "      \n",
        "      print(\"Errors: \\(errors)\")\n",
        "    }\n",
        "  }        \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcnGsWhpAFpl",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCfVTFS6AHbp",
        "colab_type": "code",
        "outputId": "69e54f63-36d2-4f48-fccf-852a3158316f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "train(features, targets, epochs: 100, learningRate: 0.01)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\r\n",
            "Train loss: 0.8829083619477006 \r\n",
            "Accuracy: 0.5\r\n",
            "Errors: 88.29083619477007\n",
            "Epoch: 10\n",
            "Train loss: 0.6743747883272507 \n",
            "Accuracy: 0.56\n",
            "Errors: 67.43747883272508\n",
            "Epoch: 20\n",
            "Train loss: 0.5959861768943281 \n",
            "Accuracy: 0.77\n",
            "Errors: 59.59861768943281\n",
            "Epoch: 30\n",
            "Train loss: 0.5343250043715391 \n",
            "Accuracy: 0.86\n",
            "Errors: 53.432500437153905\n",
            "Epoch: 40\n",
            "Train loss: 0.4858426265239904 \n",
            "Accuracy: 0.92\n",
            "Errors: 48.58426265239904\n",
            "Epoch: 50\n",
            "Train loss: 0.44713395487810526 \n",
            "Accuracy: 0.93\n",
            "Errors: 44.71339548781052\n",
            "Epoch: 60\n",
            "Train loss: 0.4157170299358725 \n",
            "Accuracy: 0.93\n",
            "Errors: 41.57170299358725\n",
            "Epoch: 70\n",
            "Train loss: 0.38981249980610433 \n",
            "Accuracy: 0.93\n",
            "Errors: 38.981249980610436\n",
            "Epoch: 80\n",
            "Train loss: 0.3681404088268396 \n",
            "Accuracy: 0.93\n",
            "Errors: 36.81404088268396\n",
            "Epoch: 90\n",
            "Train loss: 0.34977007224210405 \n",
            "Accuracy: 0.93\n",
            "Errors: 34.9770072242104\n",
            "Epoch: 100\n",
            "Train loss: 0.33401522011459245 \n",
            "Accuracy: 0.93\n",
            "Errors: 33.40152201145924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXJPnQuRtf8I",
        "colab_type": "text"
      },
      "source": [
        "Compare with python calculation\n",
        "\n",
        "========== Epoch 80 ==========\n",
        "\n",
        "Train loss:  0.35459973368161973\n",
        "\n",
        "Accuracy:  0.94\n",
        "\n",
        "========== Epoch 90 ==========\n",
        "\n",
        "Train loss:  0.3379273658879921\n",
        "\n",
        "Accuracy:  0.94"
      ]
    }
  ]
}