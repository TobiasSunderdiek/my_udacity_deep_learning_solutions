{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gradient_descent.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/squeze/my_udacity_deep_learning_solutions/blob/master/intro-neural-networks/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z0eKFKr8neG",
        "colab_type": "text"
      },
      "source": [
        "# Implementing the gradient descent algorithm\n",
        "\n",
        "\n",
        "This notebook is based on the udacity deep learning nanodegree exercise for gradient descent, which can be found here:\n",
        "\n",
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-neural-networks/gradient-descent/GradientDescent.ipynb\n",
        "\n",
        "The original version is implemented with python and numpy, I try to implement it with swift-only as an exercise to learn swift."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb5V5Sb09V4W",
        "colab_type": "text"
      },
      "source": [
        "## Loading dataset from github\n",
        "The original dataset is located here:\n",
        "\n",
        "https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-neural-networks/gradient-descent/data.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "outputId": "24011565-63c1-4c78-f4b6-c0a1030fe676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import Foundation\n",
        "\n",
        "let url = \"https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-neural-networks/gradient-descent/data.csv\"\n",
        "\n",
        "// author of this query function: https://gist.github.com/groz/85b95f663f79ba17946269ea65c2c0f4\n",
        "func query(address: String) -> String {\n",
        "    let url = URL(string: address)\n",
        "    let semaphore = DispatchSemaphore(value: 0)\n",
        "    \n",
        "    var result: String = \"\"\n",
        "    \n",
        "    let task = URLSession.shared.dataTask(with: url!) {(data, response, error) in\n",
        "        result = String(data: data!, encoding: String.Encoding.utf8)!\n",
        "        semaphore.signal()\n",
        "    }\n",
        "    \n",
        "    task.resume()\n",
        "    semaphore.wait()\n",
        "    return result\n",
        "}\n",
        "\n",
        "let rawData = query(address: url)\n",
        "let rows = rawData.components(separatedBy: \"\\n\")\n",
        "let featuresAndTargets = rows.map({ $0.components(separatedBy: \",\") })\n",
        "print(featuresAndTargets)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[\"0.78051\", \"-0.063669\", \"1\"], [\"0.28774\", \"0.29139\", \"1\"], [\"0.40714\", \"0.17878\", \"1\"], [\"0.2923\", \"0.4217\", \"1\"], [\"0.50922\", \"0.35256\", \"1\"], [\"0.27785\", \"0.10802\", \"1\"], [\"0.27527\", \"0.33223\", \"1\"], [\"0.43999\", \"0.31245\", \"1\"], [\"0.33557\", \"0.42984\", \"1\"], [\"0.23448\", \"0.24986\", \"1\"], [\"0.0084492\", \"0.13658\", \"1\"], [\"0.12419\", \"0.33595\", \"1\"], [\"0.25644\", \"0.42624\", \"1\"], [\"0.4591\", \"0.40426\", \"1\"], [\"0.44547\", \"0.45117\", \"1\"], [\"0.42218\", \"0.20118\", \"1\"], [\"0.49563\", \"0.21445\", \"1\"], [\"0.30848\", \"0.24306\", \"1\"], [\"0.39707\", \"0.44438\", \"1\"], [\"0.32945\", \"0.39217\", \"1\"], [\"0.40739\", \"0.40271\", \"1\"], [\"0.3106\", \"0.50702\", \"1\"], [\"0.49638\", \"0.45384\", \"1\"], [\"0.10073\", \"0.32053\", \"1\"], [\"0.69907\", \"0.37307\", \"1\"], [\"0.29767\", \"0.69648\", \"1\"], [\"0.15099\", \"0.57341\", \"1\"], [\"0.16427\", \"0.27759\", \"1\"], [\"0.33259\", \"0.055964\", \"1\"], [\"0.53741\", \"0.28637\", \"1\"], [\"0.19503\", \"0.36879\", \"1\"], [\"0.40278\", \"0.035148\", \"1\"], [\"0.21296\", \"0.55169\", \"1\"], [\"0.48447\", \"0.56991\", \"1\"], [\"0.25476\", \"0.34596\", \"1\"], [\"0.21726\", \"0.28641\", \"1\"], [\"0.67078\", \"0.46538\", \"1\"], [\"0.3815\", \"0.4622\", \"1\"], [\"0.53838\", \"0.32774\", \"1\"], [\"0.4849\", \"0.26071\", \"1\"], [\"0.37095\", \"0.38809\", \"1\"], [\"0.54527\", \"0.63911\", \"1\"], [\"0.32149\", \"0.12007\", \"1\"], [\"0.42216\", \"0.61666\", \"1\"], [\"0.10194\", \"0.060408\", \"1\"], [\"0.15254\", \"0.2168\", \"1\"], [\"0.45558\", \"0.43769\", \"1\"], [\"0.28488\", \"0.52142\", \"1\"], [\"0.27633\", \"0.21264\", \"1\"], [\"0.39748\", \"0.31902\", \"1\"], [\"0.5533\", \"1\", \"0\"], [\"0.44274\", \"0.59205\", \"0\"], [\"0.85176\", \"0.6612\", \"0\"], [\"0.60436\", \"0.86605\", \"0\"], [\"0.68243\", \"0.48301\", \"0\"], [\"1\", \"0.76815\", \"0\"], [\"0.72989\", \"0.8107\", \"0\"], [\"0.67377\", \"0.77975\", \"0\"], [\"0.78761\", \"0.58177\", \"0\"], [\"0.71442\", \"0.7668\", \"0\"], [\"0.49379\", \"0.54226\", \"0\"], [\"0.78974\", \"0.74233\", \"0\"], [\"0.67905\", \"0.60921\", \"0\"], [\"0.6642\", \"0.72519\", \"0\"], [\"0.79396\", \"0.56789\", \"0\"], [\"0.70758\", \"0.76022\", \"0\"], [\"0.59421\", \"0.61857\", \"0\"], [\"0.49364\", \"0.56224\", \"0\"], [\"0.77707\", \"0.35025\", \"0\"], [\"0.79785\", \"0.76921\", \"0\"], [\"0.70876\", \"0.96764\", \"0\"], [\"0.69176\", \"0.60865\", \"0\"], [\"0.66408\", \"0.92075\", \"0\"], [\"0.65973\", \"0.66666\", \"0\"], [\"0.64574\", \"0.56845\", \"0\"], [\"0.89639\", \"0.7085\", \"0\"], [\"0.85476\", \"0.63167\", \"0\"], [\"0.62091\", \"0.80424\", \"0\"], [\"0.79057\", \"0.56108\", \"0\"], [\"0.58935\", \"0.71582\", \"0\"], [\"0.56846\", \"0.7406\", \"0\"], [\"0.65912\", \"0.71548\", \"0\"], [\"0.70938\", \"0.74041\", \"0\"], [\"0.59154\", \"0.62927\", \"0\"], [\"0.45829\", \"0.4641\", \"0\"], [\"0.79982\", \"0.74847\", \"0\"], [\"0.60974\", \"0.54757\", \"0\"], [\"0.68127\", \"0.86985\", \"0\"], [\"0.76694\", \"0.64736\", \"0\"], [\"0.69048\", \"0.83058\", \"0\"], [\"0.68122\", \"0.96541\", \"0\"], [\"0.73229\", \"0.64245\", \"0\"], [\"0.76145\", \"0.60138\", \"0\"], [\"0.58985\", \"0.86955\", \"0\"], [\"0.73145\", \"0.74516\", \"0\"], [\"0.77029\", \"0.7014\", \"0\"], [\"0.73156\", \"0.71782\", \"0\"], [\"0.44556\", \"0.57991\", \"0\"], [\"0.85275\", \"0.85987\", \"0\"], [\"0.51912\", \"0.62359\", \"0\"], [\"\"]]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1cKth9KpfSW",
        "colab_type": "text"
      },
      "source": [
        "## Sigmoid activation function\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "Swift is strongly typed, types for input/output must be given. Sigmoid must handle a tensor of floating point values. Choose double.\n",
        "\n",
        "Swift uses local and external parameter names. To avoid giving the name of the parameter by calling the function, use _ for the external name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdTW4VQ29WZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import TensorFlow\n",
        "\n",
        "func mySigmoid(_ x: Tensor<Double>) -> Tensor<Double> {\n",
        "  return 1 / (1 + exp(-x))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM5vboL7Jq62",
        "colab_type": "text"
      },
      "source": [
        "### Test\n",
        "Test with original tensorflow sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnZa-m8K9ZrX",
        "colab_type": "code",
        "outputId": "3ffd7d6e-f2be-4ee9-8aba-d40b6ab3aa18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "let original = sigmoid(Tensor([1.0]))\n",
        "let myResult = mySigmoid(Tensor([1.0]))\n",
        "print(original[0], myResult[0])\n",
        "assert(original[0] == myResult[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7310585786300049 0.7310585786300049\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qcupn4EUj6A",
        "colab_type": "text"
      },
      "source": [
        "## Output (prediction) formula\n",
        "$$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2 + b)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWP3Fw8JJReL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func myOutputFormula(_ inputs: Tensor<Double>, _ weights: Tensor<Double>, _ bias: Tensor<Double>) -> Tensor<Double> {\n",
        "  return mySigmoid(matmul(inputs, weights) + bias)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IW3IHgDddb9P",
        "colab_type": "text"
      },
      "source": [
        "### Test\n",
        "\n",
        "**Input**: 3 rows with 2 featues each, features arranged in columns, one row is one sample, shape is [3,2]\n",
        "\n",
        "**Nodes**: let nodes in the neural net be 2 within a hidden layer  (not necessary for the multiplication here, but only for the example data)\n",
        "\n",
        "**Weights**: 3 input row * 2 nodes = 6 weights are needed, shape is [2,3]\n",
        "\n",
        "**Bias**: a bias is broadcasted for each node of the n-1 layer, shape is [3,1]\n",
        "\n",
        "**Result**: 3x2-matrix multiplied with 2x3-matrix results in 3x3-matrix\n",
        "\n",
        "**Calculation**:\n",
        "\n",
        "*   matrix multiplication\n",
        "\n",
        ">$0.1 * 1 + 1.1 * 4 = 4.5$\n",
        "\n",
        ">$0.1 * 2 + 1.1 * 5 = 5.7$\n",
        "\n",
        ">$0.1 * 3 + 1.1 * 6 = 6.9$\n",
        "\n",
        ">$0.2 * 1 + 2.2 * 4 = 9$\n",
        "\n",
        ">$0.2 * 2 + 2.2 * 5 = 11.4$\n",
        "\n",
        ">$0.2 * 3 + 2.2 * 6 = 13.8$\n",
        "\n",
        ">$0.3 * 1 + 3.3 * 4 = 13.5$\n",
        "\n",
        ">$0.3 * 2 + 3.3 * 5 = 17.1$\n",
        "\n",
        ">$0.3 * 3 + 3.3 * 6 = 20.7$\n",
        "\n",
        "$$\n",
        "\\left[\\begin{array}{cc} \n",
        "0.1 & 1.1\\\\\n",
        "0.2 & 2.2 \\\\\n",
        "0.3 & 3.3\n",
        "\\end{array}\\right]\n",
        "\\left[\\begin{array}{cc} \n",
        "1 & 2 & 3\\\\ \n",
        "4 & 5 & 6\n",
        "\\end{array}\\right]\n",
        "=\n",
        "\\left[\\begin{array}{cc} \n",
        "4.5 & 5.7 & 6.9\\\\\n",
        "9 &11.4 & 13.8\\\\\n",
        "13.5 & 17.1 & 20.7\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "* broadcast bias\n",
        "\n",
        ">As described in [1] adding a bias vector to a matrix is allowed and is called broadcasting\n",
        "\n",
        "$$\n",
        "\\left[\\begin{array}{cc} \n",
        "4.5 & 5.7 & 6.9\\\\\n",
        "9 &11.4 & 13.8\\\\\n",
        "13.5 & 17.1 & 20.7\n",
        "\\end{array}\\right]\n",
        "+\n",
        "\\left[\\begin{array}{cc}\n",
        "0.01 \\\\\n",
        "0.02 \\\\\n",
        "0.03\n",
        "\\end{array}\\right]\n",
        "=\n",
        "\\left[\\begin{array}{cc} \n",
        "4.51 & 5.72 & 6.93\\\\\n",
        "9.01 &11.42 & 13.83\\\\\n",
        "13.51 & 17.12 & 20.73\n",
        "\\end{array}\\right]\n",
        "$$\n",
        "\n",
        "* sigmoid function\n",
        "\n",
        ">$\\sigma(4.51) = 0.9891211899829261$\n",
        "\n",
        "\n",
        "$$\\sigma\\left[\\begin{array}{cc} \n",
        "4.51 & 5.72 & 6.93\\\\\n",
        "9.01 &11.42 & 13.83\\\\\n",
        "13.51 & 17.12 & 20.73\n",
        "\\end{array}\\right]\n",
        "=\n",
        "\\left[\\begin{array}{cc} \n",
        "0.9891211899829261 & 0.9967310104383614 & 0.9990229546827732 \\\\\n",
        " 0.9998778330705631 & 0.9999890263210334 & 0.9999990143859467\\\\\n",
        " 0.9999986426840268 & 0.9999999632820475 & 0.9999999990067114\n",
        "\\end{array}\\right]$$\n",
        "\n",
        "[1] http://www.deeplearningbook.org/contents/linear_algebra.html, page 32"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t53H0vyvVXCj",
        "colab_type": "code",
        "outputId": "415d5db7-d096-4d3f-99dd-2ab6d772f960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "let features = Tensor<Double>([[0.1, 1.1], [0.2, 2.2], [0.3, 3.3]])\n",
        "let weights = Tensor<Double>([[1, 2, 3], [4, 5, 6]])\n",
        "let bias = Tensor<Double>([0.01, 0.02, 0.03])\n",
        "\n",
        "print(features.shape)\n",
        "print(weights.shape)\n",
        "print(bias.shape)\n",
        "\n",
        "let myOutput = myOutputFormula(features, weights, bias)\n",
        "print(myOutput)\n",
        "print(myOutput.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[3, 2]\r\n",
            "[2, 3]\r\n",
            "[3]\r\n",
            "[0.9891211899829261]\r\n",
            "[[0.9891211899829261, 0.9967310104383614, 0.9990229546827732],\r\n",
            " [0.9998778330705631, 0.9999890263210334, 0.9999990143859467],\r\n",
            " [0.9999986426840268, 0.9999999632820475, 0.9999999990067114]]\r\n",
            "[3, 3]\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD_6SeB-Ar3Z",
        "colab_type": "text"
      },
      "source": [
        "## Error function\n",
        "\n",
        "Log-loss (in this case equals to Cross-Entroy) for binary classification\n",
        "$$Error(y, \\hat{y}) = - y \\log(\\hat{y}) - (1-y) \\log(1-\\hat{y})$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCsbyp8JVyX8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func myErrorFormula(_ y: Double, _ ŷ: Double) -> Double {\n",
        "  return -(y * log(ŷ)) - ((1-y) * log(1-ŷ))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdUwYfSKJgYa",
        "colab_type": "text"
      },
      "source": [
        "### Test\n",
        "\n",
        "y: target value of each input data e.g. 0, 1, 0\n",
        "\n",
        "ŷ: prediction e.g. 0.7, 0.4, 0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsVkN8D4IfTz",
        "colab_type": "code",
        "outputId": "d24157f5-4640-41a0-b9e8-c92517605ffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "let ŷ = 0.9\n",
        "let y = 1.0\n",
        "myErrorFormula(y, ŷ)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10536051565782628\n"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aiSHPAxjZ39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}