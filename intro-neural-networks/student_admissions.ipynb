{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "student_admissions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "swift",
      "display_name": "Swift"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TobiasSunderdiek/my_udacity_deep_learning_solutions/blob/master/intro-neural-networks/student_admissions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z0eKFKr8neG",
        "colab_type": "text"
      },
      "source": [
        "# Predicting student admissions\n",
        "\n",
        "\n",
        "This notebook is based on the udacity deep learning nanodegree exercise for gradient descent, which can be found here:\n",
        "\n",
        "https://github.com/udacity/deep-learning-v2-pytorch/blob/master/intro-neural-networks/student-admissions/StudentAdmissions.ipynb\n",
        "\n",
        "The original version is implemented with python and numpy, I try to implement it with swift-only as an exercise to learn swift.\n",
        "\n",
        "Additionally to the implementation, to better understand the underlying calculus, I do some derivation of the formula for updating the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwJXqDDcL7Xl",
        "colab_type": "text"
      },
      "source": [
        "## Math - backpropagation with mean-squared-error as error function within perceptron\n",
        "\n",
        "The underlying single-layer perceptron [1] of this notebook can be shown as follows:\n",
        "\n",
        "\n",
        "<p><a href=\"https://commons.wikimedia.org/wiki/File:Perceptron.svg#/media/File:Perceptron.svg\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/3/31/Perceptron.svg\" alt=\"Perceptron.svg\" height=\"353\" width=\"440\"></a><br>By <a href=\"https://en.wikipedia.org/wiki/User:Mat_the_w\" class=\"extiw\" title=\"wikipedia:User:Mat the w\">Mat the w</a> at <a href=\"https://en.wikipedia.org/wiki/\" class=\"extiw\" title=\"wikipedia:\">English Wikipedia</a>, <a href=\"https://creativecommons.org/licenses/by-sa/3.0\" title=\"Creative Commons Attribution-Share Alike 3.0\">CC BY-SA 3.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=23766733\">Link</a></p>\n",
        "\n",
        "In our case, we  have inputs $greScaled$, $gpaScaled$ and $encodedRank$ and their weights. Due to the hot-encoding of rank, we have input size of 7 ($w_1 x_1 + w_2 x_2...+w_7 x_7$), so n=7, and we add a bias b and o is output (= $\\hat{y}$): $$\\hat{y} = f(w_1 x_1 + w_2 x_2... + b)$$\n",
        "The function f is our activation function, which is sigmoid: $$\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2... + b)$$\n",
        "\n",
        "#### Updating the weights\n",
        "\n",
        "Generally, gradient descent[2] describes the change in each weight for multilayer perceptrons as:\n",
        "\n",
        "$$\\Delta w_{ji} (n) = -\\eta\\frac{\\partial\\mathcal{E}(n)}{\\partial v_j(n)} y_i(n)$$\n",
        "\n",
        "$n$ is the node, in this case we have a single-layer perceptron which has 1 node\n",
        "\n",
        "$v_j(n)$ is the weighted sum of the input connections in the node, in this case $w_1 x_1 + w_2 x_2...w_7 x_7 + b$, including bias\n",
        "\n",
        "$y_i(n)$ is the output of the previous node, the i-th node, in this case we don't have a previous node, our inputs are the general inputs of the perceptron $x_i$\n",
        "\n",
        "$\\mathcal{E}(n)$ is the error function, which in this case is the mean-squared-error $\\mathcal{E}(n)=\\frac{1}{2}\\sum_i (y-\\hat{y})_i^2(n)$\n",
        "\n",
        "$\\eta$ is the learning rate\n",
        "\n",
        "Given the information above,  with one node $i=1$:\n",
        "\n",
        "$$\\Delta w_{j} (n) = -\\eta\\frac{\\partial (\\frac{1}{2} (y-\\hat{y})^2(n))}{\\partial v_j} x$$\n",
        "\n",
        "\n",
        "How much of the total error can be influenced by an individual $v_j$ is calculated by getting the partial derivative of the loss-function with respect to $v_j$:\n",
        "\n",
        "$$\\frac{\\partial (\\frac{1}{2} (y-\\hat{y})^2)}{\\partial v_j}$$\n",
        "\n",
        "\n",
        "As $\\hat{y} = \\sigma(w_1 x_1 + w_2 x_2... + b)$, and $v_j = w_1 x_1 + w_2 x_2... + b$, we have $\\hat{y} = \\sigma(v_j)$\n",
        "\n",
        "$$\\frac{\\partial (\\frac{1}{2} (y-\\sigma(v_j))^2)}{\\partial v_j}$$\n",
        "\n",
        "Now, we use the chain-rule $\\frac{\\partial}{\\partial z}p(q(z)) = \\frac{\\partial p}{\\partial q} \\frac{\\partial q}{\\partial z}$, where $q(z) = y-\\sigma(v_j)$ and $p=\\frac{1}{2}q^2$:\n",
        "\n",
        "$$\\frac{\\partial \\frac{1}{2}q^2}{\\partial q} \\frac{\\partial (y-\\sigma(v_j))}{\\partial v_j}=q\\frac{\\partial }{\\partial v_j}(y-\\sigma(v_j))=q(0- (\\sigma(v_j)(1-\\sigma(v_j))))=(y-\\sigma(v_j))(-(\\sigma(v_j)(1-\\sigma(v_j)))=-(y-\\hat{y})(\\sigma^\\prime(v_j))$$\n",
        "\n",
        "Put this back in:\n",
        "\n",
        "$$\\Delta w_{j} (n) = \\eta(y-\\hat{y})(\\sigma^\\prime(v_j)) x$$\n",
        "\n",
        "This is the derivative of the formula for the weight update, same for bias:\n",
        "\n",
        " $$w_i \\longrightarrow w_i + \\eta(y-\\hat{y})(\\sigma^\\prime(v_i)) x_i$$\n",
        "\n",
        "[1] https://en.wikipedia.org/wiki/Perceptron\n",
        "\n",
        "[2] https://en.wikipedia.org/wiki/Multilayer_perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb5V5Sb09V4W",
        "colab_type": "text"
      },
      "source": [
        "## Loading dataset from github\n",
        "The original dataset is located here:\n",
        "\n",
        "https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-neural-networks/student-admissions/student_data.csv\n",
        "\n",
        "which originally came from: http://www.ats.ucla.edu/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZRlD4utdPuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import Foundation\n",
        "\n",
        "let url = \"https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-neural-networks/student-admissions/student_data.csv\"\n",
        "\n",
        "// author of this query function: https://gist.github.com/groz/85b95f663f79ba17946269ea65c2c0f4\n",
        "func query(address: String) -> String {\n",
        "    let url = URL(string: address)\n",
        "    let semaphore = DispatchSemaphore(value: 0)\n",
        "    \n",
        "    var result: String = \"\"\n",
        "    \n",
        "    let task = URLSession.shared.dataTask(with: url!) {(data, response, error) in\n",
        "        result = String(data: data!, encoding: String.Encoding.utf8)!\n",
        "        semaphore.signal()\n",
        "    }\n",
        "    \n",
        "    task.resume()\n",
        "    semaphore.wait()\n",
        "    return result\n",
        "}\n",
        "\n",
        "let rawData = query(address: url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBCYDv0gPiHI",
        "colab_type": "text"
      },
      "source": [
        "### Convert data\n",
        "- Make data an array of Tensors\n",
        "\n",
        "- Drop first row with column header\n",
        "\n",
        "\n",
        "|admit|\tgre|\tgpa|\trank|\n",
        "|-|-|-|-|\n",
        "|0\t|380\t|3.61\t|3|\n",
        "|1\t|660\t|3.67\t|3|\n",
        "|1\t|800\t|4.00\t|1|\n",
        "|1\t|640\t|3.19\t|4|\n",
        "|0\t|520\t|2.93\t|4|\n",
        "\n",
        "- One-hot encode the numerical values of column rank, which contains values from 1-4:\n",
        "\n",
        "> 1 -> [0.0, 1.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "> 3 -> [0.0, 0.0, 0.0, 1.0, 0.0]\n",
        "\n",
        "> 4 -> [0.0, 0.0, 0.0, 0.0, 1.0]\n",
        "\n",
        "|admit|gre|\tgpa|\tencodedRank|\n",
        "|-|-|-|-|\n",
        "|0\t|380\t|3.61\t|0.0 0.0 0.0 1.0 0.0|\n",
        "|1\t|660\t|3.67\t|0.0 0.0 0.0 1.0 0.0|\n",
        "|1\t|800\t|4.00\t|0.0 1.0 0.0 0.0 0.0|\n",
        "|1\t|640\t|3.19\t|0.0 0.0 0.0 0.0 1.0|\n",
        "|0\t|520\t|2.93\t|0.0 0.0 0.0 0.0 1.0|\n",
        "\n",
        "> **Difference to the pandas get_dummies-function used in the original solution: oneHotAtIndices starts counting indices at 0 and this results in 5 instead of 4 columns for the encoded rank**\n",
        "\n",
        "-  Scaling the data\n",
        "\n",
        "> *gre* has values between 200-800, *gpa* has values between 1.0-4.0, so the features have to be scaled to be normalized.\n",
        "\n",
        "> Fit features into range within 0-1 by dividing *gre*/800 and *gpa*/4\n",
        "\n",
        "|admit|greScaled|\tgpaScaled|\tencodedRank|\n",
        "|-|-|-|-|\n",
        "|0\t|0.475\t|0.9025\t|0.0 0.0 0.0 1.0 0.0|\n",
        "|1\t|0.825\t|0.9175\t|0.0 0.0 0.0 1.0 0.0|\n",
        "|1\t|1.0\t|1.0\t|0.0 1.0 0.0 0.0 0.0|\n",
        "|1\t|0.8\t|0.7975\t|0.0 0.0 0.0 0.0 1.0|\n",
        "|0\t|0.65\t|0.7325|0.0 0.0 0.0 0.0 1.0|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QThtcqTjMRyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import TensorFlow\n",
        "\n",
        "let rows = rawData.components(separatedBy: \"\\n\")\n",
        "let featuresAndTargetsAsString = rows.dropFirst().map({ $0.components(separatedBy: \",\") }).filter {$0[0] != \"\"}              \n",
        "var data = [Tensor<Double>]()\n",
        "for featureWithTarget in featuresAndTargetsAsString {\n",
        "  let admit = Double(featureWithTarget[0])!\n",
        "  let gre = Double(featureWithTarget[1])!\n",
        "  let gpa = Double(featureWithTarget[2])!\n",
        "  let rank = Int32(featureWithTarget[3])!\n",
        "  \n",
        "  let greScaled = gre/800\n",
        "  let gpaScaled = gpa/4\n",
        "  let encodedRank = Tensor<Double>(oneHotAtIndices: Tensor<Int32>(rank), depth:5)\n",
        "  \n",
        "  let admitGreGpa = Tensor<Double>([admit, greScaled, gpaScaled])\n",
        "  let feature = admitGreGpa.concatenated(with: encodedRank)\n",
        "  data.append(feature)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFXhUrU9895a",
        "colab_type": "text"
      },
      "source": [
        "### Split into training and test set\n",
        "Testing set will be 10% of total size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8GPA_xB8zE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "let dataShuffled = data.shuffled()\n",
        "let ninetyPercentCount = data.count * 90 / 100\n",
        "\n",
        "let dataTrain = dataShuffled.prefix(upTo: ninetyPercentCount)\n",
        "let dataTest = dataShuffled.suffix(from: ninetyPercentCount)\n",
        "\n",
        "let featuresTrain = dataTrain.map { $0.slice(lowerBounds: [1], upperBounds: [8]) }\n",
        "let targetsTrain = dataTrain.map { $0.slice(lowerBounds: [0], upperBounds: [1]).scalars[0] }\n",
        "\n",
        "let featuresTest = dataTest.map { $0.slice(lowerBounds: [1], upperBounds: [8]) }\n",
        "let targetsTest = dataTest.map { $0.slice(lowerBounds: [0], upperBounds: [1]) }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY-ZPkcRVC0M",
        "colab_type": "text"
      },
      "source": [
        "### Output (prediction) formula, sigmoid\n",
        "This functions are the same as in my `gradient_descent.ipynb` in\n",
        "\n",
        "https://github.com/TobiasSunderdiek/my_udacity_deep_learning_solutions/blob/master/intro-neural-networks/gradient_descent.ipynb\n",
        "\n",
        "Calculation of the output in the udacity version of this notebook is different than in my version: a bias is added here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2FWf5CuVjpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func mySigmoid(_ x: Tensor<Double>) -> Tensor<Double> {\n",
        "  return 1 / (1 + exp(-x))\n",
        "}\n",
        "func myOutputFormula(_ features: Tensor<Double>, _ weights: Tensor<Double>, _ bias: Tensor<Double>) -> Double {\n",
        "  let res = mySigmoid((features * weights).sum() + bias)\n",
        "  return res.scalar!\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RHfS4zURnNx",
        "colab_type": "text"
      },
      "source": [
        "### Error function, error term formula, gradient descent step, backpropagation, sigmoid prime\n",
        "\n",
        "Error formula (result of error formula) is not used in original version. I use MSE for calculating loss as in original version and put it in error formula function instead. I sum up the errors and calculate mean during and after iteration of epoch.\n",
        "\n",
        "For updating the weights, the gradient descent step of the backpropagation, mean-squared error is used.  In the original version, the update is calculated within each iteration in `error_term_formula` and performed after each epoch-iteration. Here it is performed at each step of iteration in `myUpdateWeights`.\n",
        "\n",
        "During this step, the bias is updated.\n",
        "\n",
        "I choose this approach just to keep the train function the same as in the `gradient_descent.ipynb`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5aiSHPAxjZ39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func sigmoidPrime(_ x: Tensor<Double>) -> Tensor<Double> {\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "}\n",
        "func myErrorFormula(_ y: Double, _ output: Double) -> Double {\n",
        "  return pow(y-output, 2)\n",
        "}\n",
        "func myUpdateWeights(_ features: Tensor<Double>, _ targets: Double, _ weights: Tensor<Double>, _ bias: Tensor<Double>, _ learningRate: Double) -> (Tensor<Double>, Tensor<Double>) {\n",
        "  let delta = learningRate * (targets - myOutputFormula(features, weights, bias)) * sigmoidPrime(features).sum() //sum here because sigmoidPrime give shape of 7, we need a scalar here\n",
        "  let updatedWeights = weights + delta * features\n",
        "  let updatedBias = bias + delta\n",
        "\n",
        "  return (updatedWeights, updatedBias)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqgcSRLE_8W_",
        "colab_type": "text"
      },
      "source": [
        "## Training function\n",
        "Initialization of weights in the course\n",
        "\n",
        "`weights = np.random.normal(scale=1 / n_features**.5, size=n_features)`\n",
        "\n",
        "is different than in my version\n",
        "\n",
        "`var weights = Tensor<Double>(randomNormal: [n_features])`\n",
        "\n",
        "Training function is same as in https://github.com/TobiasSunderdiek/my_udacity_deep_learning_solutions/blob/master/intro-neural-networks/gradient_descent.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWYCXa5S_-34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func train(_ features: [Tensor<Double>], _ targets: [Double], epochs: Int, learningRate: Double) { //make learningRate Double because swift's won't multiply Double with Float\n",
        "  let numberRecords = Double(features.count)\n",
        "  var weights = Tensor<Double>(randomNormal: features[0].shape)\n",
        "  var bias = Tensor<Double>.zero\n",
        "  var lastLoss = Double.infinity\n",
        "  \n",
        "  for epoch in 0...epochs {\n",
        "    var errors = 0.0\n",
        "    var correctPredictions = 0.0\n",
        "    var prediction = 0.0\n",
        "\n",
        "    for (x, y) in zip(features, targets) {\n",
        "      let output = myOutputFormula(x, weights, bias)\n",
        "      errors += myErrorFormula(y, output)\n",
        "      (weights, bias) = myUpdateWeights(x, y, weights, bias, learningRate)\n",
        "\n",
        "      if (output > 0.5) {\n",
        "        prediction = 1.0\n",
        "      } else {\n",
        "        prediction = 0.0\n",
        "      }\n",
        "      \n",
        "      if (prediction == y) {\n",
        "        correctPredictions+=1\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    let loss = errors / numberRecords\n",
        "\n",
        "    if epoch % (epochs / 10) == 0 {\n",
        "      print(\"Epoch: \\(epoch)\")\n",
        "      \n",
        "      let warning = lastLoss < loss ? \"WARNING - Loss increasing\" : \"\"\n",
        "      print(\"Train loss: \\(loss) \\(warning)\")\n",
        "      lastLoss = loss\n",
        "      \n",
        "      let accuracy = correctPredictions / numberRecords\n",
        "      print(\"Accuracy: \\(accuracy)\")\n",
        "      \n",
        "      print(\"Errors: \\(errors)\")\n",
        "    }\n",
        "  }        \n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcnGsWhpAFpl",
        "colab_type": "text"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCfVTFS6AHbp",
        "colab_type": "code",
        "outputId": "4053953d-9452-41d5-b06f-8f544f471308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "train(featuresTrain, targetsTrain, epochs: 1000, learningRate: 0.5)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\r\n",
            "Train loss: 0.26075068142563673 \r\n",
            "Accuracy: 0.6333333333333333\r\n",
            "Errors: 93.87024531322922\n",
            "Epoch: 100\n",
            "Train loss: 0.2538984042839295 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554221463\n",
            "Epoch: 200\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 300\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 400\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 500\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 600\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 700\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 800\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 900\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n",
            "Epoch: 1000\n",
            "Train loss: 0.25389840428215077 \n",
            "Accuracy: 0.6472222222222223\n",
            "Errors: 91.40342554157428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXJPnQuRtf8I",
        "colab_type": "text"
      },
      "source": [
        "Compare with python calculation\n",
        "\n",
        "Epoch: 0\n",
        "Train loss:  0.27151046424991654\n",
        "\n",
        "Epoch: 100\n",
        "Train loss:  0.20925670061926063\n",
        "\n",
        "Epoch: 900\n",
        "Train loss:  0.203646868060691\n",
        "\n",
        "Prediction accuracy: 0.725"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvERTaqT_o5r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}